{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3978be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPProcessor, PromptClassifier\n",
    "from medclip.prompts import generate_chexpert_class_prompts\n",
    "from medclip.dataset import ZeroShotImageDataset, ZeroShotImageCollator\n",
    "from medclip.evaluator import Evaluator\n",
    "\n",
    "from notebooks.Beisong_prompts import my_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a643f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/liubeisong/Desktop/2025_Fall/Small_Data/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7688a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = os.path.join(path, \"images/images_normalized\")\n",
    "PROJECTIONS_PATH = os.path.join(path, \"indiana_projections.csv\")\n",
    "REPORTS_PATH = os.path.join(path, \"indiana_reports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40ce9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj = pd.read_csv(PROJECTIONS_PATH)\n",
    "df_rep = pd.read_csv(REPORTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24aa5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_proj, df_rep, on=\"uid\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571c4803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'filename', 'projection', 'MeSH', 'Problems', 'image',\n",
       "       'indication', 'comparison', 'findings', 'impression'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8219c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3851 unique patients 7466 total rows\n",
      "7466 unique images\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[\"uid\"].nunique(), \"unique patients\", len(df_merged), \"total rows\")\n",
    "print(df_merged[\"filename\"].nunique(), \"unique images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187c3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we process the problems column, we can split them into a list\n",
    "# also remove duplicates\n",
    "SEP_PATTERN = r\"[|,;/]\"\n",
    "df_merged[\"Problem_List\"] = (\n",
    "    df_merged[\"Problems\"]\n",
    "    .str.lower()\n",
    "    .str.split(SEP_PATTERN)\n",
    "    .apply(lambda lst: list(dict.fromkeys([x.strip() for x in lst if x.strip()])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6df7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['atelectasis', 'cardiomegaly', 'consolidation', 'edema', 'pleural effusion']\n",
    "df_merged[\"disease_count\"] = df_merged[\"Problem_List\"].apply(\n",
    "    lambda lst: sum(\n",
    "        any(disease in x for x in lst) for disease in CLASSES\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53f5a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_disease_df = df_merged[df_merged[\"disease_count\"] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38a91236",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_disease_df[\"Disease\"] = single_disease_df[\"Problem_List\"].apply(\n",
    "    lambda lst: next((d for d in CLASSES if any(d in x for x in lst)), None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79426b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disease\n",
       "cardiomegaly        442\n",
       "atelectasis         433\n",
       "pleural effusion    117\n",
       "consolidation        23\n",
       "edema                12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_disease_df[\"Disease\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da2e4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_disease_df.to_csv(\"single_disease_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e9e41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5c7c334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class counts after capping:\n",
      "Disease\n",
      "atelectasis         50\n",
      "cardiomegaly        50\n",
      "consolidation       23\n",
      "edema               12\n",
      "pleural effusion    50\n",
      "Name: count, dtype: int64\n",
      "Saved to: local_data/chexpert-multiclass-capped-meta.csv | shape=(185, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/n8w5xxxj4wx8zsnfn3mkw8080000gn/T/ipykernel_41816/2346589020.py:13: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=min(len(g), MAX_PER_CLASS), random_state=42))\n"
     ]
    }
   ],
   "source": [
    "### Multiclass zero-shot (5 CheXpert classes)\n",
    "\n",
    "MAX_PER_CLASS = 50\n",
    "\n",
    "# Map to paths and class names (lowercase to match prompts)\n",
    "df_mc = single_disease_df.copy()\n",
    "df_mc['Disease'] = df_mc['Disease'].str.lower()\n",
    "df_mc['imgpath'] = df_mc['filename'].map(lambda f: f if os.path.isabs(f) else os.path.join(IMG_PATH, f))\n",
    "\n",
    "# Cap per-class to at most 50 (take all if < 50)\n",
    "df_bal = (\n",
    "    df_mc.groupby('Disease', group_keys=False)\n",
    "         .apply(lambda g: g.sample(n=min(len(g), MAX_PER_CLASS), random_state=42))\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# One-hot in CLASSES order\n",
    "for c in CLASSES:\n",
    "    df_bal[c] = (df_bal['Disease'] == c).astype(int)\n",
    "\n",
    "meta_mc = df_bal[['imgpath'] + CLASSES]\n",
    "\n",
    "# Save for ZeroShotImageDataset\n",
    "out_csv = Path('local_data/chexpert-multiclass-capped-meta.csv')\n",
    "out_csv.parent.mkdir(exist_ok=True)\n",
    "meta_mc.to_csv(out_csv)\n",
    "\n",
    "print(\"Per-class counts after capping:\")\n",
    "print(df_bal['Disease'].value_counts().reindex(CLASSES))\n",
    "print(f\"Saved to: {out_csv} | shape={meta_mc.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c83f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "afd70147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 8 num of prompts for Atelectasis from total 210\n",
      "sample 8 num of prompts for Cardiomegaly from total 15\n",
      "sample 8 num of prompts for Consolidation from total 192\n",
      "sample 8 num of prompts for Edema from total 18\n",
      "sample 8 num of prompts for Pleural Effusion from total 54\n",
      "Prompt counts per class:\n",
      "{'atelectasis': 8, 'cardiomegaly': 8, 'consolidation': 8, 'edema': 8, 'pleural effusion': 8}\n"
     ]
    }
   ],
   "source": [
    "# 2) Generate prompts for five CheXpert classes (avoid disease words in negatives)\n",
    "chex_prompts = generate_chexpert_class_prompts(n=8)\n",
    "\n",
    "# Build lowercase -> positive prompt mapping\n",
    "cls_prompts_mc = {c: chex_prompts[c.title()] for c in CLASSES}\n",
    "\n",
    "\n",
    "print(\"Prompt counts per class:\")\n",
    "print({k: len(v) for k, v in cls_prompts_mc.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14a2d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt counts per class:\n",
      "{'atelectasis': 5, 'cardiomegaly': 5, 'consolidation': 5, 'edema': 5, 'pleural effusion': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt counts per class:\")\n",
    "print({k: len(v) for k, v in my_prompts.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8651792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from ./local_data/chexpert-multiclass-capped-meta.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medclip_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 6\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
      "Prompt keys: ['atelectasis', 'cardiomegaly', 'consolidation', 'edema', 'pleural effusion']\n"
     ]
    }
   ],
   "source": [
    "# 3) Create multiclass dataset/loader\n",
    "\n",
    "mc_dataset = ZeroShotImageDataset(\n",
    "    datalist=['chexpert-multiclass-capped'],\n",
    "    class_names=CLASSES\n",
    ")\n",
    "\n",
    "mc_collator = ZeroShotImageCollator(\n",
    "    mode='multiclass',\n",
    "    cls_prompts=cls_prompts_mc\n",
    ")\n",
    "\n",
    "mc_loader = DataLoader(\n",
    "    mc_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=mc_collator,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print('Batches:', len(mc_loader))\n",
    "sample = next(iter(mc_loader))\n",
    "print(sample['pixel_values'].shape, sample['labels'].shape)\n",
    "print('Prompt keys:', list(sample['prompt_inputs'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "785d1353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/medclip_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cpu\n",
      "load model weight from: pretrained/medclip-vit\n",
      "Multiclass classifier ready\n"
     ]
    }
   ],
   "source": [
    "# 4) Init model + PromptClassifier (ensemble)\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel.from_pretrained(vision_model='vit', device='cpu')\n",
    "clf_mc = PromptClassifier(model, ensemble=True).to('cpu')\n",
    "# clf_mc.to('mps')\n",
    "clf_mc.eval()\n",
    "print('Multiclass classifier ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6171344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 6/6 [00:48<00:00,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS (Multiclass - 5 CheXpert)\n",
      "==================================================\n",
      "acc                 : 0.6108\n",
      "precision           : 0.6084\n",
      "recall              : 0.5243\n",
      "f1-score            : 0.5414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% # 5) Evaluate (multiclass) \n",
    "evaluator_mc = Evaluator( medclip_clf=clf_mc, eval_dataloader=mc_loader, mode='multiclass' ) \n",
    "results_mc = evaluator_mc.evaluate() \n",
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"EVALUATION RESULTS (Multiclass - 5 CheXpert)\") \n",
    "print(\"=\"*50)\n",
    "for metric, value in results_mc.items(): \n",
    "    if metric not in ['pred', 'labels']: \n",
    "        if isinstance(value, float): print(f\"{metric:20s}: {value:.4f}\") \n",
    "        else: print(f\"{metric:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6959336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'accuracy': 0.6108, 'macro_f1': 0.5414, 'weighted_f1': 0.6075}\n",
      "\n",
      "Per-class metrics:\n",
      "           class  precision   recall  f1-score  support  correct  auc_ovr\n",
      "pleural effusion   0.660377 0.700000  0.679612     50.0       35 0.817630\n",
      "    cardiomegaly   0.569231 0.740000  0.643478     50.0       37 0.886370\n",
      "   consolidation   1.000000 0.434783  0.606061     23.0       10 0.745303\n",
      "     atelectasis   0.630435 0.580000  0.604167     50.0       29 0.804444\n",
      "           edema   0.181818 0.166667  0.173913     12.0        2 0.750482\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "CLASSES = ['atelectasis','cardiomegaly','consolidation','edema','pleural effusion']\n",
    "\n",
    "# --- logits -> probs & predictions ---\n",
    "logits = torch.tensor(results_mc['pred'])          # [N, C]\n",
    "probs  = torch.softmax(logits, dim=1).numpy()      # [N, C]\n",
    "y_pred = probs.argmax(axis=1)                      # [N]\n",
    "\n",
    "# --- labels: accept [N] indices or [N,C] one-hot ---\n",
    "labels = np.array(results_mc['labels'])\n",
    "y_true = labels if labels.ndim == 1 else labels.argmax(axis=1)\n",
    "\n",
    "# --- per-class precision/recall/F1/support ---\n",
    "rep = classification_report(\n",
    "    y_true, y_pred, target_names=CLASSES, digits=4, output_dict=True\n",
    ")\n",
    "per_class_df = (\n",
    "    pd.DataFrame(rep).T\n",
    "      .loc[CLASSES, ['precision','recall','f1-score','support']]\n",
    "      .reset_index().rename(columns={'index':'class'})\n",
    ")\n",
    "\n",
    "# --- per-class AUC (OvR); NaN if a class has no positives ---\n",
    "y_true_oh = np.zeros_like(probs)\n",
    "y_true_oh[np.arange(len(y_true)), y_true] = 1\n",
    "try:\n",
    "    auc_per_class = roc_auc_score(y_true_oh, probs, multi_class='ovr', average=None)\n",
    "except Exception:\n",
    "    auc_per_class = np.array([np.nan]*len(CLASSES))\n",
    "auc_df = pd.DataFrame({'class': CLASSES, 'auc_ovr': auc_per_class})\n",
    "\n",
    "# --- correct counts (TP per class) from confusion matrix diagonal ---\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(len(CLASSES)))\n",
    "correct_counts = np.diag(cm)\n",
    "\n",
    "# --- combine everything ---\n",
    "metrics_df = (\n",
    "    per_class_df\n",
    "      .merge(auc_df, on='class', how='left')\n",
    "      .assign(correct=correct_counts)\n",
    "      .loc[:, ['class','precision','recall','f1-score','support','correct','auc_ovr']]\n",
    "      .sort_values('f1-score', ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- overall summaries ---\n",
    "overall = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred),\n",
    "    'macro_f1': rep['macro avg']['f1-score'],\n",
    "    'weighted_f1': rep['weighted avg']['f1-score']\n",
    "}\n",
    "\n",
    "print(\"Overall:\", {k: round(v, 4) for k, v in overall.items()})\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c0405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9193dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'accuracy': 0.6108108108108108, 'macro_f1': 0.5414460455121072, 'weighted_f1': 0.6075090776336027}\n",
      "\n",
      "Per-class metrics:\n",
      "           class  precision   recall  f1-score  support  auc_ovr\n",
      "pleural effusion   0.660377 0.700000  0.679612     50.0 0.818963\n",
      "    cardiomegaly   0.569231 0.740000  0.643478     50.0 0.886370\n",
      "   consolidation   1.000000 0.434783  0.606061     23.0 0.757381\n",
      "     atelectasis   0.630435 0.580000  0.604167     50.0 0.806667\n",
      "           edema   0.181818 0.166667  0.173913     12.0 0.750000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    ")\n",
    "\n",
    "CLASSES = ['atelectasis','cardiomegaly','consolidation','edema','pleural effusion']\n",
    "\n",
    "# --- Get predictions & labels ---\n",
    "logits = torch.tensor(results_mc['pred'])           # [N, C]\n",
    "probs  = torch.softmax(logits, dim=1).numpy()       # [N, C]\n",
    "y_pred = probs.argmax(axis=1)                       # [N]\n",
    "\n",
    "labels = np.array(results_mc['labels'])\n",
    "if labels.ndim == 2:\n",
    "    y_true = labels.argmax(axis=1)\n",
    "else:\n",
    "    y_true = labels\n",
    "\n",
    "# --- Per-class precision/recall/F1/support ---\n",
    "report_dict = classification_report(\n",
    "    y_true, y_pred, target_names=CLASSES, digits=4, output_dict=True\n",
    ")\n",
    "per_class_df = (\n",
    "    pd.DataFrame(report_dict)\n",
    "      .T.loc[CLASSES, ['precision','recall','f1-score','support']]\n",
    "      .reset_index()\n",
    "      .rename(columns={'index':'class'})\n",
    ")\n",
    "\n",
    "# --- Overall & macro/micro summaries (optional) ---\n",
    "overall = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred),\n",
    "    'macro_f1': report_dict['macro avg']['f1-score'],\n",
    "    'weighted_f1': report_dict['weighted avg']['f1-score']\n",
    "}\n",
    "overall\n",
    "\n",
    "# --- Confusion matrix (counts & row-normalized) ---\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(len(CLASSES)))\n",
    "cm_df = pd.DataFrame(cm, index=[f\"true:{c}\" for c in CLASSES],\n",
    "                        columns=[f\"pred:{c}\" for c in CLASSES])\n",
    "\n",
    "cm_norm = (cm.astype(float) / cm.sum(axis=1, keepdims=True).clip(min=1))\n",
    "cm_norm_df = pd.DataFrame(cm_norm, index=[f\"true:{c}\" for c in CLASSES],\n",
    "                                   columns=[f\"pred:{c}\" for c in CLASSES])\n",
    "\n",
    "# --- Per-class ROC-AUC (one-vs-rest) ---\n",
    "# Need one-hot y_true for AUC\n",
    "y_true_oh = np.zeros_like(probs)\n",
    "y_true_oh[np.arange(len(y_true)), y_true] = 1\n",
    "\n",
    "# returns array of AUC per class in CLASSES order\n",
    "try:\n",
    "    auc_per_class = roc_auc_score(y_true_oh, probs, multi_class='ovr', average=None)\n",
    "    auc_df = pd.DataFrame({'class': CLASSES, 'auc_ovr': auc_per_class})\n",
    "except Exception as e:\n",
    "    # e.g., if a class has no positive samples in y_true\n",
    "    auc_df = pd.DataFrame({'class': CLASSES, 'auc_ovr': [np.nan]*len(CLASSES)})\n",
    "\n",
    "# --- Nice combined table ---\n",
    "metrics_df = (\n",
    "    per_class_df\n",
    "      .merge(auc_df, on='class', how='left')\n",
    "      .sort_values('f1-score', ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Confusion matrix (for correct counts)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(len(CLASSES)))\n",
    "correct_counts = np.diag(cm)\n",
    "\n",
    "# Add as new column to metrics_df\n",
    "metrics_df['correct'] = correct_counts\n",
    "\n",
    "# Reorder for clarity\n",
    "metrics_df = metrics_df[['class', 'precision', 'recall', 'f1-score', 'support', 'correct', 'auc_ovr']]\n",
    "\n",
    "print(\"\\nPer-class metrics (with correct counts):\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d509fe",
   "metadata": {},
   "source": [
    "we only want cases with unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ab7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "target_class = \"Pleural Effusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70982c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "# positives dataset\n",
    "pos_df = df_merged[df_merged[\"Problem_List\"].apply(lambda x: target_class.lower() in x)].copy()\n",
    "pos_df[\"imgpath\"] = pos_df[\"filename\"].map(lambda f: f if os.path.isabs(f) else os.path.join(IMG_PATH, f))\n",
    "pos_df[target_class] = 1\n",
    "pos_df[\"Normal\"] = 0\n",
    "print(len(pos_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c85d887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "# negatives dataset\n",
    "neg_df = df_merged[df_merged['Problems'] == \"normal\"].copy()\n",
    "neg_df[\"imgpath\"] = neg_df[\"filename\"].map(lambda f: f if os.path.isabs(f) else os.path.join(IMG_PATH, f))\n",
    "neg_df = neg_df.sample(n=min(len(neg_df), len(pos_df)), random_state=42)\n",
    "neg_df[target_class] = 0\n",
    "neg_df[\"Normal\"] = 1\n",
    "print(len(neg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b256fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 572 (pos=286, neg=286). Missing files: 0\n",
      "\n",
      "Saved metadata to: local_data/Pleural Effusion-test-meta.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta_df = pd.concat([\n",
    "    pos_df[['imgpath',target_class,'Normal']],\n",
    "    neg_df[['imgpath',target_class,'Normal']]\n",
    "], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Quick sanity: count missing files\n",
    "missing = (~meta_df['imgpath'].apply(os.path.exists)).sum()\n",
    "print(f\"Total images: {len(meta_df)} (pos={len(pos_df)}, neg={len(neg_df)}). Missing files: {missing}\")\n",
    "\n",
    "output_path = Path(f'local_data/{target_class}-test-meta.csv')\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "meta_df.to_csv(output_path)\n",
    "print(f\"\\nSaved metadata to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fa4c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 7 num of prompts for Atelectasis from total 210\n",
      "sample 7 num of prompts for Cardiomegaly from total 15\n",
      "sample 7 num of prompts for Consolidation from total 192\n",
      "sample 7 num of prompts for Edema from total 18\n",
      "sample 7 num of prompts for Pleural Effusion from total 54\n",
      "Pleural Effusion prompts: 7\n",
      "Normal prompts: 3\n"
     ]
    }
   ],
   "source": [
    "# Prepare prompts: Cardiomegaly vs Normal\n",
    "# We will use CheXpert cardiomegaly prompts and simple \"No Finding\" style for Normal\n",
    "\n",
    "chex_prompts = generate_chexpert_class_prompts(n=7)\n",
    "target_prompts = chex_prompts[target_class]\n",
    "\n",
    "normal_prompts = neg_prompts[target_class]\n",
    "\n",
    "\n",
    "cls_prompts_dict = {\n",
    "    target_class: target_prompts,\n",
    "    'Normal': normal_prompts,\n",
    "}\n",
    "\n",
    "print(f'{target_class} prompts:', len(target_prompts))\n",
    "print('Normal prompts:', len(normal_prompts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cardio prompts\n",
    "for x in target_prompts:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset and dataloader for cardiomegaly\n",
    "class_names = [target_class, 'Normal']\n",
    "\n",
    "# Create dataset from our metadata\n",
    "dataset = ZeroShotImageDataset(\n",
    "    datalist=[f'{target_class}-test'],  \n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Collator with prompts (binary)\n",
    "collator = ZeroShotImageCollator(\n",
    "    mode='binary',\n",
    "    cls_prompts=cls_prompts_dict\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"DataLoader ready: {len(loader)} batches\")\n",
    "# Test one batch\n",
    "sample_batch = next(iter(loader))\n",
    "print(sample_batch['pixel_values'].shape, sample_batch['labels'].shape)\n",
    "print('Prompt inputs keys:', list(sample_batch['prompt_inputs'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba50832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and classifier\n",
    "processor = MedCLIPProcessor()\n",
    "# Use resnet by default; switch to 'vit' if preferred\n",
    "model = MedCLIPModel.from_pretrained(vision_model='resnet', device='mps')\n",
    "clf = PromptClassifier(model, ensemble=False)\n",
    "clf.to('mps')\n",
    "clf.eval()\n",
    "print('Model and classifier ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# Binary classification (Cardiomegaly vs Normal)\n",
    "evaluator = Evaluator(\n",
    "    medclip_clf=clf,\n",
    "    eval_dataloader=loader,\n",
    "    mode='binary'\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"EVALUATION RESULTS ({target_class} vs Normal)\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in results.items():\n",
    "    if metric not in ['pred', 'labels']:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{metric:20s}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric:20s}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f34ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results['pred']\n",
    "pred_scores = torch.tensor(pred).sigmoid().numpy()\n",
    "print('Predicted class counts:', (pred_scores.argmax(1)==0).sum(), (pred_scores.argmax(1)==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff086a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medclip_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
