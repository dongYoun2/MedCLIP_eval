{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e21a64",
   "metadata": {},
   "source": [
    "# NIH Few-Shot (One-Shot) Classification with MedCLIP\n",
    "\n",
    "This notebook fine-tunes a lightweight linear head on top of a frozen MedCLIP image encoder using one/few-shot samples per class and evaluates on the held-out test split generated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce25693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1\n",
      "Device: mps\n",
      "Train CSV: /Users/zitongluo/Library/Mobile Documents/com~apple~CloudDocs/硕士相关/2025Fall/Learning from small data/MedCLIP_eval/local_data/nih-sampled-meta-train.csv\n",
      "Test  CSV: /Users/zitongluo/Library/Mobile Documents/com~apple~CloudDocs/硕士相关/2025Fall/Learning from small data/MedCLIP_eval/local_data/nih-sampled-meta-test.csv\n"
     ]
    }
   ],
   "source": [
    "# ========= 1) Setup & Config =========\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from IPython.display import display\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPProcessor\n",
    "from medclip.modeling_medclip import SuperviseClassifier\n",
    "\n",
    "# Prefer Apple Silicon GPU or CUDA when available\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "THIS_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd().resolve()\n",
    "TRAIN_CSV = (THIS_DIR / \"local_data\" / \"nih-sampled-meta-train.csv\").resolve()\n",
    "TEST_CSV = (THIS_DIR / \"local_data\" / \"nih-sampled-meta-test.csv\").resolve()\n",
    "IMAGE_ROOT = THIS_DIR  # imgpath column already stores repo-relative paths\n",
    "\n",
    "CHEXPERT5 = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Pleural Effusion\"]\n",
    "\n",
    "VISION_MODEL = \"vit\"                      # or \"resnet\"\n",
    "SHOT_PER_CLASS = 5                          # one-shot by default; increase (e.g., 5) for few-shot\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Train CSV:\", TRAIN_CSV)\n",
    "print(\"Test  CSV:\", TEST_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3240d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size after filtering: 2000\n",
      "disease\n",
      "Atelectasis         400\n",
      "Cardiomegaly        400\n",
      "Consolidation       400\n",
      "Edema               400\n",
      "Pleural Effusion    400\n",
      "Name: count, dtype: int64\n",
      "Test size after filtering: 5000\n",
      "disease\n",
      "Atelectasis         1000\n",
      "Cardiomegaly        1000\n",
      "Consolidation       1000\n",
      "Edema               1000\n",
      "Pleural Effusion    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ========= 2) Load train/test metadata =========\n",
    "def resolve_path(p: str) -> Path:\n",
    "    raw = Path(p)\n",
    "    return raw.resolve() if raw.is_absolute() else (IMAGE_ROOT / raw).resolve()\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "for name, df in ((\"train\", train_df), (\"test\", test_df)):\n",
    "    if \"disease\" not in df.columns or \"imgpath\" not in df.columns:\n",
    "        raise KeyError(f\"{name} CSV must contain 'disease' and 'imgpath' columns.\")\n",
    "    df[\"disease\"] = df[\"disease\"].astype(str)\n",
    "    df[\"img_abs_path\"] = df[\"imgpath\"].apply(lambda p: str(resolve_path(p)))\n",
    "    exists = df[\"img_abs_path\"].apply(lambda p: Path(p).exists())\n",
    "    missing = len(df) - exists.sum()\n",
    "    if missing:\n",
    "        print(f\"[WARN] {missing} files missing in {name} split; they will be dropped.\")\n",
    "        display(df.loc[~exists, [\"Image Index\", \"img_abs_path\"]].head(10))\n",
    "    df.drop(index=df.index[~exists], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Keep disease-only five-class subset\n",
    "train_df = train_df.loc[train_df[\"disease\"].isin(CHEXPERT5)].reset_index(drop=True)\n",
    "test_df = test_df.loc[test_df[\"disease\"].isin(CHEXPERT5)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train size after filtering:\", len(train_df))\n",
    "print(train_df[\"disease\"].value_counts().sort_index())\n",
    "print(\"Test size after filtering:\", len(test_df))\n",
    "print(test_df[\"disease\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65c0e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot train size: 25\n",
      "disease\n",
      "Atelectasis         5\n",
      "Cardiomegaly        5\n",
      "Consolidation       5\n",
      "Edema               5\n",
      "Pleural Effusion    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ========= 3) Sample one/few-shot training subset =========\n",
    "def build_few_shot(df: pd.DataFrame, shots: int, seed: int) -> pd.DataFrame:\n",
    "    if shots < 1:\n",
    "        raise ValueError(\"shots must be >= 1\")\n",
    "    sampled_frames = []\n",
    "    for label, group in df.groupby(\"disease\"):\n",
    "        take = min(shots, len(group))\n",
    "        sampled = group.sample(n=take, random_state=seed, replace=False)\n",
    "        sampled_frames.append(sampled)\n",
    "    few = pd.concat(sampled_frames, ignore_index=True)\n",
    "    few = few.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return few\n",
    "\n",
    "few_shot_train_df = build_few_shot(train_df, SHOT_PER_CLASS, RANDOM_STATE)\n",
    "if few_shot_train_df.empty:\n",
    "    raise RuntimeError(\"Few-shot training set is empty; check SHOT_PER_CLASS and data filtering.\")\n",
    "\n",
    "print(\"Few-shot train size:\", len(few_shot_train_df))\n",
    "print(few_shot_train_df[\"disease\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920b9a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/medclip_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 25 | Batches: 7\n",
      "Test samples: 5000 | Batches: 1250\n"
     ]
    }
   ],
   "source": [
    "# ========= 4) Dataset & Dataloader helpers =========\n",
    "class NIHSingleLabelDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, class_to_idx: Dict[str, int], processor: MedCLIPProcessor):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        with Image.open(row[\"img_abs_path\"]) as img:\n",
    "            image = img.convert(\"RGB\")\n",
    "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = processed[\"pixel_values\"].squeeze(0)\n",
    "        label_idx = self.class_to_idx[row[\"disease\"]]\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"label\": torch.tensor(label_idx, dtype=torch.long),\n",
    "            \"path\": row[\"img_abs_path\"],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    paths = [item[\"path\"] for item in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"paths\": paths}\n",
    "\n",
    "processor = MedCLIPProcessor()\n",
    "\n",
    "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CHEXPERT5)}\n",
    "IDX_TO_CLASS = {idx: cls for cls, idx in CLASS_TO_IDX.items()}\n",
    "\n",
    "train_dataset = NIHSingleLabelDataset(few_shot_train_df, CLASS_TO_IDX, processor)\n",
    "test_dataset = NIHSingleLabelDataset(test_df, CLASS_TO_IDX, processor)\n",
    "\n",
    "train_batch_size = max(1, min(BATCH_SIZE, len(train_dataset)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset), \"| Batches:\", len(train_loader))\n",
    "print(\"Test samples:\", len(test_dataset), \"| Batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ed273f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/medclip_eval/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/zitongluo/Library/Mobile Documents/com~apple~CloudDocs/硕士相关/2025Fall/Learning from small data/MedCLIP_eval/medclip/modeling_medclip.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME), map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to mps\n",
      "load model weight from: pretrained/medclip-vit\n",
      "Trainable parameters in head: 3845\n"
     ]
    }
   ],
   "source": [
    "# ========= 5) Load MedCLIP & build supervised head =========\n",
    "base_model = MedCLIPModel.from_pretrained(vision_model=VISION_MODEL, device=DEVICE)\n",
    "\n",
    "vision_encoder = base_model.vision_model\n",
    "vision_encoder.to(DEVICE)\n",
    "setattr(vision_encoder, \"device\", torch.device(DEVICE))\n",
    "\n",
    "classifier_input_dim = 768 if VISION_MODEL == \"vit\" else 512\n",
    "supervised = SuperviseClassifier(\n",
    "    vision_model=vision_encoder,\n",
    "    num_class=len(CHEXPERT5),\n",
    "    input_dim=classifier_input_dim,\n",
    "    mode=\"multiclass\",\n",
    ").to(DEVICE)\n",
    "\n",
    "for param in supervised.model.parameters():\n",
    "    param.requires_grad = False\n",
    "supervised.model.eval()\n",
    "\n",
    "supervised.fc.reset_parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(supervised.fc.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"Trainable parameters in head:\", sum(p.numel() for p in supervised.fc.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71b8fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | loss=1.6638\n",
      "Epoch 02/50 | loss=1.5094\n",
      "Epoch 03/50 | loss=1.3824\n",
      "Epoch 04/50 | loss=1.2939\n",
      "Epoch 05/50 | loss=1.2062\n",
      "Epoch 06/50 | loss=1.1311\n",
      "Epoch 07/50 | loss=1.0592\n",
      "Epoch 08/50 | loss=0.9972\n",
      "Epoch 09/50 | loss=0.9454\n",
      "Epoch 10/50 | loss=0.8945\n",
      "Epoch 11/50 | loss=0.8503\n",
      "Epoch 12/50 | loss=0.7997\n",
      "Epoch 13/50 | loss=0.7691\n",
      "Epoch 14/50 | loss=0.7410\n",
      "Epoch 15/50 | loss=0.7039\n",
      "Epoch 16/50 | loss=0.6692\n",
      "Epoch 17/50 | loss=0.6429\n",
      "Epoch 18/50 | loss=0.6182\n",
      "Epoch 19/50 | loss=0.5963\n",
      "Epoch 20/50 | loss=0.5726\n",
      "Epoch 21/50 | loss=0.5512\n",
      "Epoch 22/50 | loss=0.5294\n",
      "Epoch 23/50 | loss=0.5114\n",
      "Epoch 24/50 | loss=0.4939\n",
      "Epoch 25/50 | loss=0.4748\n",
      "Epoch 26/50 | loss=0.4588\n",
      "Epoch 27/50 | loss=0.4452\n",
      "Epoch 28/50 | loss=0.4277\n",
      "Epoch 29/50 | loss=0.4164\n",
      "Epoch 30/50 | loss=0.4010\n",
      "Epoch 31/50 | loss=0.3905\n",
      "Epoch 32/50 | loss=0.3784\n",
      "Epoch 33/50 | loss=0.3638\n",
      "Epoch 34/50 | loss=0.3524\n",
      "Epoch 35/50 | loss=0.3441\n",
      "Epoch 36/50 | loss=0.3328\n",
      "Epoch 37/50 | loss=0.3262\n",
      "Epoch 38/50 | loss=0.3149\n",
      "Epoch 39/50 | loss=0.3063\n",
      "Epoch 40/50 | loss=0.2977\n",
      "Epoch 41/50 | loss=0.2884\n",
      "Epoch 42/50 | loss=0.2810\n",
      "Epoch 43/50 | loss=0.2753\n",
      "Epoch 44/50 | loss=0.2680\n",
      "Epoch 45/50 | loss=0.2619\n",
      "Epoch 46/50 | loss=0.2570\n",
      "Epoch 47/50 | loss=0.2516\n",
      "Epoch 48/50 | loss=0.2438\n",
      "Epoch 49/50 | loss=0.2346\n",
      "Epoch 50/50 | loss=0.2319\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "# ========= 6) Fine-tune linear head (few-shot) =========\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    supervised.train()\n",
    "    supervised.model.eval()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for batch in train_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = supervised(pixel_values=pixel_values, labels=labels, return_loss=True)\n",
    "        loss = outputs[\"loss_value\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * pixel_values.size(0)\n",
    "        count += pixel_values.size(0)\n",
    "    epoch_loss = running_loss / max(1, count)\n",
    "    loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch:02d}/{NUM_EPOCHS} | loss={epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "227be9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0000 (25 samples)\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     Atelectasis     1.0000    1.0000    1.0000         5\n",
      "    Cardiomegaly     1.0000    1.0000    1.0000         5\n",
      "   Consolidation     1.0000    1.0000    1.0000         5\n",
      "           Edema     1.0000    1.0000    1.0000         5\n",
      "Pleural Effusion     1.0000    1.0000    1.0000         5\n",
      "\n",
      "        accuracy                         1.0000        25\n",
      "       macro avg     1.0000    1.0000    1.0000        25\n",
      "    weighted avg     1.0000    1.0000    1.0000        25\n",
      "\n",
      "Test accuracy: 0.4804 (5000 samples)\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     Atelectasis     0.4963    0.4700    0.4828      1000\n",
      "    Cardiomegaly     0.6348    0.6310    0.6329      1000\n",
      "   Consolidation     0.3059    0.2190    0.2552      1000\n",
      "           Edema     0.4611    0.4980    0.4788      1000\n",
      "Pleural Effusion     0.4624    0.5840    0.5161      1000\n",
      "\n",
      "        accuracy                         0.4804      5000\n",
      "       macro avg     0.4721    0.4804    0.4732      5000\n",
      "    weighted avg     0.4721    0.4804    0.4732      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========= 7) Evaluation =========\n",
    "def predict(dataloader):\n",
    "    supervised.eval()\n",
    "    supervised.model.eval()\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    paths = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = supervised(pixel_values=pixel_values, labels=None, return_loss=False)\n",
    "            logits_list.append(outputs[\"logits\"].detach().cpu())\n",
    "            labels_list.append(labels.detach().cpu())\n",
    "            paths.extend(batch[\"paths\"])\n",
    "    if not logits_list:\n",
    "        return np.empty((0, len(CHEXPERT5))), np.empty((0,), dtype=int), paths\n",
    "    logits = torch.cat(logits_list, dim=0).numpy()\n",
    "    labels = torch.cat(labels_list, dim=0).numpy()\n",
    "    return logits, labels, paths\n",
    "\n",
    "train_logits, train_labels, _ = predict(train_loader)\n",
    "test_logits, test_labels, test_paths = predict(test_loader)\n",
    "\n",
    "\n",
    "def summarize(split: str, logits: np.ndarray, labels: np.ndarray):\n",
    "    if logits.shape[0] == 0:\n",
    "        print(f\"{split}: no samples to evaluate.\")\n",
    "        return None\n",
    "    preds = logits.argmax(axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"{split} accuracy: {acc:.4f} ({len(labels)} samples)\")\n",
    "    report = classification_report(labels, preds, target_names=CHEXPERT5, digits=4, zero_division=0)\n",
    "    print(report)\n",
    "    return preds\n",
    "\n",
    "train_preds = summarize(\"Train\", train_logits, train_labels)\n",
    "test_preds = summarize(\"Test\", test_logits, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9917351b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>prob_Atelectasis</th>\n",
       "      <th>prob_Cardiomegaly</th>\n",
       "      <th>prob_Consolidation</th>\n",
       "      <th>prob_Edema</th>\n",
       "      <th>prob_Pleural_Effusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>0.251663</td>\n",
       "      <td>0.152476</td>\n",
       "      <td>0.127819</td>\n",
       "      <td>0.184181</td>\n",
       "      <td>0.283860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Consolidation</td>\n",
       "      <td>Edema</td>\n",
       "      <td>0.056459</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.403942</td>\n",
       "      <td>0.433222</td>\n",
       "      <td>0.065224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Edema</td>\n",
       "      <td>Consolidation</td>\n",
       "      <td>0.080830</td>\n",
       "      <td>0.089066</td>\n",
       "      <td>0.435175</td>\n",
       "      <td>0.323160</td>\n",
       "      <td>0.071768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>Edema</td>\n",
       "      <td>0.132853</td>\n",
       "      <td>0.125199</td>\n",
       "      <td>0.208922</td>\n",
       "      <td>0.305372</td>\n",
       "      <td>0.227654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0.089542</td>\n",
       "      <td>0.580331</td>\n",
       "      <td>0.101627</td>\n",
       "      <td>0.143153</td>\n",
       "      <td>0.085347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>Edema</td>\n",
       "      <td>0.131688</td>\n",
       "      <td>0.117355</td>\n",
       "      <td>0.228223</td>\n",
       "      <td>0.367664</td>\n",
       "      <td>0.155069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0.186596</td>\n",
       "      <td>0.279761</td>\n",
       "      <td>0.174641</td>\n",
       "      <td>0.167292</td>\n",
       "      <td>0.191709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Consolidation</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>0.121993</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.201292</td>\n",
       "      <td>0.290766</td>\n",
       "      <td>0.299401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>0.347894</td>\n",
       "      <td>0.094248</td>\n",
       "      <td>0.083082</td>\n",
       "      <td>0.104594</td>\n",
       "      <td>0.370182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Users/zitongluo/.cache/kagglehub/datasets/nih...</td>\n",
       "      <td>Edema</td>\n",
       "      <td>Edema</td>\n",
       "      <td>0.189799</td>\n",
       "      <td>0.180819</td>\n",
       "      <td>0.220937</td>\n",
       "      <td>0.240289</td>\n",
       "      <td>0.168155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path        true_label  \\\n",
       "0  /Users/zitongluo/.cache/kagglehub/datasets/nih...  Pleural Effusion   \n",
       "1  /Users/zitongluo/.cache/kagglehub/datasets/nih...     Consolidation   \n",
       "2  /Users/zitongluo/.cache/kagglehub/datasets/nih...             Edema   \n",
       "3  /Users/zitongluo/.cache/kagglehub/datasets/nih...  Pleural Effusion   \n",
       "4  /Users/zitongluo/.cache/kagglehub/datasets/nih...      Cardiomegaly   \n",
       "5  /Users/zitongluo/.cache/kagglehub/datasets/nih...      Cardiomegaly   \n",
       "6  /Users/zitongluo/.cache/kagglehub/datasets/nih...       Atelectasis   \n",
       "7  /Users/zitongluo/.cache/kagglehub/datasets/nih...     Consolidation   \n",
       "8  /Users/zitongluo/.cache/kagglehub/datasets/nih...  Pleural Effusion   \n",
       "9  /Users/zitongluo/.cache/kagglehub/datasets/nih...             Edema   \n",
       "\n",
       "         pred_label  prob_Atelectasis  prob_Cardiomegaly  prob_Consolidation  \\\n",
       "0  Pleural Effusion          0.251663           0.152476            0.127819   \n",
       "1             Edema          0.056459           0.041152            0.403942   \n",
       "2     Consolidation          0.080830           0.089066            0.435175   \n",
       "3             Edema          0.132853           0.125199            0.208922   \n",
       "4      Cardiomegaly          0.089542           0.580331            0.101627   \n",
       "5             Edema          0.131688           0.117355            0.228223   \n",
       "6      Cardiomegaly          0.186596           0.279761            0.174641   \n",
       "7  Pleural Effusion          0.121993           0.086548            0.201292   \n",
       "8  Pleural Effusion          0.347894           0.094248            0.083082   \n",
       "9             Edema          0.189799           0.180819            0.220937   \n",
       "\n",
       "   prob_Edema  prob_Pleural_Effusion  \n",
       "0    0.184181               0.283860  \n",
       "1    0.433222               0.065224  \n",
       "2    0.323160               0.071768  \n",
       "3    0.305372               0.227654  \n",
       "4    0.143153               0.085347  \n",
       "5    0.367664               0.155069  \n",
       "6    0.167292               0.191709  \n",
       "7    0.290766               0.299401  \n",
       "8    0.104594               0.370182  \n",
       "9    0.240289               0.168155  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========= 8) Preview predictions =========\n",
    "if test_preds is not None and len(test_preds) > 0:\n",
    "    probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()\n",
    "    records = []\n",
    "    for idx, (path, true_idx, pred_idx) in enumerate(zip(test_paths, test_labels, test_preds)):\n",
    "        row = {\n",
    "            \"img_path\": path,\n",
    "            \"true_label\": IDX_TO_CLASS[int(true_idx)],\n",
    "            \"pred_label\": IDX_TO_CLASS[int(pred_idx)],\n",
    "        }\n",
    "        for class_idx, class_name in enumerate(CHEXPERT5):\n",
    "            row[f\"prob_{class_name.replace(' ', '_')}\"] = float(probs[idx, class_idx])\n",
    "        records.append(row)\n",
    "    preview_df = pd.DataFrame(records)\n",
    "    display(preview_df.head(10))\n",
    "else:\n",
    "    print(\"No test predictions available to preview.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00014fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medclip_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
