{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c51c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4419fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/dongyoun/miniconda3/envs/medclip_eval/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPProcessor, PromptClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec297d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/transformers/modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/dongyoun/projects/dongYoun2/MedCLIP_eval/medclip/modeling_medclip.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(input_dir, constants.WEIGHTS_NAME), map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to mps\n",
      "load model weight from: pretrained/medclip-resnet\n"
     ]
    }
   ],
   "source": [
    "model = MedCLIPModel.from_pretrained(vision_model='resnet', device='mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6bfcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "image = Image.open('example_data/view1_frontal.jpg')\n",
    "inputs = processor(text=[\"a cat\"], images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Move only the tensors to your model's device\n",
    "inputs = {k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "          for k, v in inputs.items()}\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78fee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to mps\n",
      "load model weight from: pretrained/medclip-resnet\n",
      "dict_keys(['img_embeds', 'text_embeds', 'logits', 'loss_value', 'logits_per_text'])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "# prepare for the demo image and texts\n",
    "processor = MedCLIPProcessor()\n",
    "image = Image.open('example_data/view1_frontal.jpg')\n",
    "inputs = processor(\n",
    "    text=[\"lungs remain severely hyperinflated with upper lobe emphysema\",\n",
    "        \"opacity left costophrenic angle is new since prior exam ___ represent some loculated fluid cavitation unlikely\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    "    )\n",
    "\n",
    "# Move only the tensors to your model's device\n",
    "inputs = {k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "          for k, v in inputs.items()}\n",
    "\n",
    "# pass to MedCLIP model\n",
    "model = MedCLIPModel.from_pretrained(vision_model='resnet', device=device)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df2d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d386209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongyoun/miniconda3/envs/medclip_eval/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1744320350753/work/aten/src/ATen/native/TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cpu\n",
      "load model weight from: pretrained/medclip-vit\n",
      "sample 10 num of prompts for Atelectasis from total 210\n",
      "sample 10 num of prompts for Cardiomegaly from total 15\n",
      "sample 10 num of prompts for Consolidation from total 192\n",
      "sample 10 num of prompts for Edema from total 18\n",
      "sample 10 num of prompts for Pleural Effusion from total 54\n",
      "{'logits': tensor([[0.3542, 0.4754, 0.1633, 0.2454, 0.3839]], grad_fn=<StackBackward0>), 'class_names': ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']}\n"
     ]
    }
   ],
   "source": [
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel.from_pretrained(vision_model='vit', device='cpu')\n",
    "clf = PromptClassifier(model, ensemble=True)\n",
    "clf.to('cpu')\n",
    "\n",
    "# prepare input image\n",
    "image = Image.open('example_data/view1_frontal.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# prepare input prompt texts\n",
    "from medclip.prompts import generate_chexpert_class_prompts, process_class_prompts\n",
    "cls_prompts = process_class_prompts(generate_chexpert_class_prompts(n=10))\n",
    "inputs['prompt_inputs'] = cls_prompts\n",
    "\n",
    "# make classification\n",
    "output = clf(**inputs)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medclip_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
